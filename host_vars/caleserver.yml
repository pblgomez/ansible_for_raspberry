---
the_host_name: caleserver
the_group: "{{ Raspbuntu }}"

DOMAIN: "{% for item in the_group %}{% if item.name == the_host_name %}{{ item.vars[0]['host'] }}{% endif %}{% endfor %}"
DUCKDNS_SUBDOMAINS: "{% for item in the_group %}{% if item.name == the_host_name %}{{ item.vars[0]['DUCKDNS_SUBDOMAINS'] }}{% endif %}{% endfor %}"
DUCKDNS_TOKEN: "{% for item in the_group %}{% if item.name == the_host_name %}{{ item.vars[0]['DUCKDNS_TOKEN'] }}{% endif %}{% endfor %}"
HASSDB_ROOT_PASSWD: "{% for item in the_group %}{% if item.name == the_host_name %}{{ item.vars[0]['HASSDB_ROOT_PASSWD'] }}{% endif %}{% endfor %}"
HASSDB_USER: "{% for item in the_group %}{% if item.name == the_host_name %}{{ item.vars[0]['HASSDB_USER'] }}{% endif %}{% endfor %}"
HASSDB_PASSWORD: "{% for item in the_group %}{% if item.name == the_host_name %}{{ item.vars[0]['HASSDB_PASSWORD'] }}{% endif %}{% endfor %}"

NEXTC_DB_ROOT_PASSWORD: "{% for item in the_group %}{% if item.name == the_host_name %}{{ item.vars[0]['NEXTC_DB_ROOT_PASSWORD'] }}{% endif %}{% endfor %}"
NEXTC_DB_USER: "{% for item in the_group %}{% if item.name == the_host_name %}{{ item.vars[0]['NEXTC_DB_USER'] }}{% endif %}{% endfor %}"
NEXTC_DB_PASSWORD: "{% for item in the_group %}{% if item.name == the_host_name %}{{ item.vars[0]['NEXTC_DB_PASSWORD'] }}{% endif %}{% endfor %}"

WEBDAV_USER: "{% for item in the_group %}{% if item.name == the_host_name %}{{ item.vars[0]['WEBDAV_USER'] }}{% endif %}{% endfor %}"
WEBDAV_PASSWORD: "{% for item in the_group %}{% if item.name == the_host_name %}{{ item.vars[0]['WEBDAV_PASSWORD'] }}{% endif %}{% endfor %}"

global_env_vars:
  - PUID=1000
  - PGID=1000
  - TZ={{ vault_TZ }}
appdata_path: /media/storage/config
appdata_path_cached: /media/cached_storage/config   # Because of mergerfs
video_path: /media/storage/Videos
downloads_path: /media/storage/Downloads
container_config_path: /config

containers:
  - service_name: bazarr
    active: true
    include_global_env_vars: true
    container_name: bazarr
    image: ghcr.io/linuxserver/bazarr:v1.0.0-ls134
    ports:
      - 6767:6767
    volumes:
      - "{{ appdata_path_cached }}/bazarr:/config"
      - "{{ video_path }}/Movies/:/movies"
      - "{{ video_path }}/TV/:/tv"
    depends_on:
      - radarr
      - sonarr
    healthcheck:
      test_port: "6767"
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  - service_name: bitwarden
    active: true
    include_global_env_vars: true
    container_name: bitwarden
    image: vaultwarden/server:1.21.0
    # ports:
    #   - 9085:80
    volumes:
      - "{{ appdata_path_cached }}/bitwarden:/data"
    depends_on:
      - duckdns
      - caddy
    healthcheck:
      test_port: "80"
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  - service_name: caddy
    active: true
    include_global_env_vars: true
    container_name: caddy
    image: caddy:2.4.6-alpine
    ports:
      - 80:80
      - 443:443
    volumes:
      - /var/run/docker.sock:/tmp/docker.sock:ro                            # For docker
      - "{{ appdata_path }}/caddy/Caddyfile:/etc/caddy/Caddyfile"           # Caddyfile
      - "{{ appdata_path }}/caddy/data:{{ container_config_path }}/caddy"   # certificates
      - "{{ appdata_path }}/caddy/config:{{ container_config_path }}"       # json
    environment:
      - DOMAIN={{ DOMAIN }}
    depends_on:
      - duckdns
    restart: unless-stopped

  - service_name: duckdns
    active: true
    include_global_env_vars: true
    image: ghcr.io/linuxserver/duckdns:5679a6ee-ls88
    environment:
      - SUBDOMAINS={{ DUCKDNS_SUBDOMAINS }}
      - TOKEN={{ DUCKDNS_TOKEN }}
    restart: unless-stopped

  - service_name: emby
    active: true
    include_global_env_vars: true
    image: ghcr.io/linuxserver/emby:4.6.4.0-ls101
    devices:
      - /dev/vchiq:/dev/vchiq
      - /dev/video10:/dev/video10
      - /dev/video11:/dev/video11
      - /dev/video12:/dev/video12
    ports:
      - 8096:8096
    volumes:
      - "{{ appdata_path_cached }}/emby:/config"
      - "{{ video_path }}/Movies:/movies"
      - "{{ video_path }}/TV:/tvshows"
      - /opt/vc/lib:/opt/vc/lib
    healthcheck:
      test_port: 8096
      interval: 30s
      timeout: 10s
      retries: 6
    restart: unless-stopped

  - service_name: grafana
    active: true
    image: grafana/grafana
    user: "{{ docker_compose_generator_uid }}:{{ docker_compose_generator_gid }}"
    ports:
      - 3000:3000
    networks:
      automation:
        ipv4_address: 172.92.92.97
    volumes:
      - "{{ appdata_path_cached }}/grafana:/var/lib/grafana"
    restart: unless-stopped

  - service_name: heimdall
    active: true
    include_global_env_vars: true
    image: ghcr.io/linuxserver/heimdall:2.2.2-ls149
    ports:
      - 8080:80
    volumes:
      - "{{ appdata_path }}/heimdall:/config"
    healthcheck:
      test_port: 80
      interval: 30s
      timeout: 10s
      retries: 6
    restart: unless-stopped

  - service_name: homeassistant
    active: true
    include_global_env_vars: true
    container_name: homeassistant
    image: homeassistant/raspberrypi4-homeassistant:2021.10.7
    network_mode: host
    volumes:
      - "{{ appdata_path }}/homeassistant:/config"
      - /etc/localtime:/etc/localtime:ro
      - "{{ appdata_path }}/homeassistant/docker/run:/etc/services.d/home-assistant/run"
    extra_hosts:
      - hass-db:172.92.92.92
      - homeassistant_metrics:172.92.92.94
    environment:
      - PACKAGES=iputils
    depends_on:
      - caddy
      - mosquitto
      - homeassistant_db
      - node-red
      - homeassistant_metrics
    healthcheck:
      test_port: 8123
      interval: 30s
      timeout: 10s
      retries: 6
    restart: unless-stopped

  - service_name: homeassistant_db
    active: true
    include_global_env_vars: true
    image: ghcr.io/linuxserver/mariadb:alpine-10.5.12-r0-ls39
    environment:
      - MYSQL_ROOT_PASSWORD={{ HASSDB_ROOT_PASSWD }}
      - MYSQL_DATABASE=homeassistant
      - MYSQL_USER={{ HASSDB_USER }}
      - MYSQL_PASSWORD={{ HASSDB_PASSWORD }}
    volumes:
      - "{{ appdata_path_cached }}/homeassistant_db:/config"
    ports:
      - 3306:3306
    restart: unless-stopped
    networks:
      automation:
        ipv4_address: 172.92.92.92

  - service_name: homeassistant_metrics
    active: true
    image: influxdb:1.8.5
    user: "{{ docker_compose_generator_uid }}:{{ docker_compose_generator_gid }}"
    networks:
      automation:
        ipv4_address: 172.92.92.94
    volumes:
      - "{{ appdata_path_cached }}/homeassistant_metrics:/var/lib/influxdb"
    restart: unless-stopped

  - service_name: jackett
    active: true
    include_global_env_vars: true
    container_name: jackett
    image: ghcr.io/linuxserver/jackett:v0.19.260-ls50
    ports:
      - 9117:9117
    volumes:
      - "{{ appdata_path }}/jackett:/config"
      - "{{ downloads_path }}/watch:/downloads"
    healthcheck:
      test_port: "9117"
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  - service_name: joplin-webdav
    active: true
    include_global_env_vars: true
    image: ugeek/webdav:arm-alpine
    ports:
      - 8081:80
    environment:
      - "USERNAME={{ WEBDAV_USER }}"
      - "PASSWORD={{ WEBDAV_PASSWORD }}"
    volumes:
      - "{{ appdata_path }}/joplin-webdav:/data"

  - service_name: nextcloud
    active: false
    image: nextcloud:20
    # ports:
    #   - 9999:80
    volumes:
      - "{{ appdata_path }}/nextcloud:/var/www/html"
    depends_on:
      - nextcloud_db
      - caddy
    healthcheck:
      test_port: "80"
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  - service_name: nextcloud_db
    active: false
    include_global_env_vars: true
    image: ghcr.io/linuxserver/mariadb:alpine-10.5.12-r0-ls39
    environment:
      - MYSQL_ROOT_PASSWORD={{ NEXTC_DB_ROOT_PASSWORD }}
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER={{ NEXTC_DB_USER }}
      - MYSQL_PASSWORD={{ NEXTC_DB_PASSWORD }}
    ports:
      - 3307:3306
    volumes:
      - "{{ appdata_path_cached }}/nextcloud_db:/config"
    restart: unless-stopped

  - service_name: node-red
    active: true
    include_global_env_vars: true
    image: nodered/node-red:1.3.1-minimal
    ports:
      - 1880:1880
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - "{{ appdata_path }}/node-red:/data"
      - "{{ volume_node_red_modules }}:/usr/src/node-red/node_modules"
      - /etc/timezone:/etc/timezone:ro
    depends_on:
      - mosquitto
    healthcheck:
      test_port: "1880"
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  - service_name: mosquitto
    active: true
    image: eclipse-mosquitto:2.0.13
    restart: unless-stopped
    user: "{{ docker_compose_generator_uid }}:{{ docker_compose_generator_gid }}"
    ports:
      - 1883:1883
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - "{{ appdata_path }}/mosquitto:/mosquitto/config:ro"
      - "{{ appdata_path }}/mosquitto:/mosquitto/data"

  - service_name: ombi
    active: true
    include_global_env_vars: true
    image: ghcr.io/linuxserver/ombi:v4.3.3-ls110
    ports:
      - 3579:3579
    volumes:
      - "{{ appdata_path }}/ombi:/config"
    depends_on:
      - sonarr
      - radarr
    healthcheck:
      test_port: "3579"
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  - service_name: openwrt_metrics
    active: true
    image: influxdb:1.8.5
    user: "{{ docker_compose_generator_uid }}:{{ docker_compose_generator_gid }}"
    volumes:
      - "{{ appdata_path_cached }}/openwrt_metrics/db_data:/var/lib/influxdb"
      - "{{ appdata_path }}/openwrt_metrics/influxdb.conf:/etc/influxdb/influxdb.conf:ro"
      - "{{ appdata_path }}/openwrt_metrics/types.db:/usr/share/collectd/types.db:ro"
    ports:
      - 8083:8083
      - 8086:8086
      - 25826:25826/udp
    restart: unless-stopped

  - service_name: radicale
    active: true
    restart: unless-stopped
    image: tomsquest/docker-radicale

  - service_name: rancher
    active: false
    restart: unless-stopped
    image: rancher/rancher
    container_name: rancher
    privileged: true
    depends_on:
      - caddy
    volumes:
      - "{{ appdata_path }}/rancher:/var/lib/rancher"
    ports:
      - 8080:80
    command:
      - --no-cacerts

  - service_name: radarr
    active: true
    include_global_env_vars: true
    image: ghcr.io/linuxserver/radarr:3.2.2.5080-ls120
    ports:
      - 7878:7878
    volumes:
      - "{{ appdata_path_cached }}/radarr:/config"
      - "{{ video_path }}/Movies:/movies"
      - "{{ downloads_path }}:/downloads"
    depends_on:
      - jackett
      - transmission
    healthcheck:
      test_port: "7878"
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  - service_name: radicale
    active: true
    include_global_env_vars: true
    image: tomsquest/docker-radicale:arm64.3.0.6.4
    ports:
      - 5232:5232
    volumes:
      - "{{ appdata_path }}/radicale:/data"

  - service_name: sonarr
    active: true
    include_global_env_vars: true
    image: ghcr.io/linuxserver/sonarr:3.0.6.1342-ls128
    ports:
      - 8989:8989
    volumes:
      - "{{ appdata_path_cached }}/sonarr:/config"
      - "{{ video_path }}/TV:/tv"
      - "{{ downloads_path }}:/downloads"
    depends_on:
      - jackett
      - transmission
    healthcheck:
      test_port: "8989"
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  - service_name: transmission
    active: true
    include_global_env_vars: true
    image: ghcr.io/linuxserver/transmission:3.00-r2-ls105
    ports:
      - 9091:9091
      - 51413:51413
      - 51413:51413/udp
    volumes:
      - "{{ appdata_path }}/transmission:/config"
      - "{{ downloads_path }}/:/downloads"
      - "{{ video_path }}/watch:/watch"
    healthcheck:
      test_port: "9091"
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped


# volumes
volume_node_red_modules: node-red-modules

volumes:
  - volume_name: "{{ volume_node_red_modules }}"
    active: true

networks:
  - network_name: automation
    active: true
    driver: bridge
    ipam:
      config:
        - subnet: 172.92.92.0/24
